{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10751899,"sourceType":"datasetVersion","datasetId":6668519},{"sourceId":359296,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":299187,"modelId":319779}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Amini Cocoa Contamination Challenge Second Model","metadata":{}},{"cell_type":"markdown","source":"It follows exactly the same trend as the first model.\nThis code snippet initializes a YOLOv11 model and trains it using the configuration specified in the data.yaml file. A Yolo weighted dataset is utilized.\nThe YOLOWeightedDataset class tries to balance the custom for dataset training the model.\nI followed a similar training pipeline to the one used in the Ghana Crop Disease Detection Challenge Notebook by Raphael Kiminya, making modifications specific to this dataset and task.","metadata":{}},{"cell_type":"code","source":"!pip -q install -U ultralytics iterative-stratification","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-13T14:50:20.624170Z","iopub.execute_input":"2025-05-13T14:50:20.624469Z","iopub.status.idle":"2025-05-13T14:51:35.364692Z","shell.execute_reply.started":"2025-05-13T14:50:20.624448Z","shell.execute_reply":"2025-05-13T14:51:35.363807Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport os\nfrom pathlib import Path\nimport shutil\nfrom sklearn.model_selection import train_test_split\nfrom tqdm.notebook import tqdm\nimport cv2\nimport yaml\nimport matplotlib.pyplot as plt\nfrom ultralytics import YOLO\nimport multiprocessing\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport random\nfrom datetime import datetime\nimport time\nfrom glob import glob\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nfrom PIL import Image\nimport torch\nimport numpy as np\nfrom ultralytics import RTDETR\nfrom ultralytics.data.build import YOLODataset\nimport ultralytics.data.build as build\ndevice='cuda'\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T14:51:35.366237Z","iopub.execute_input":"2025-05-13T14:51:35.366483Z","iopub.status.idle":"2025-05-13T14:51:39.677284Z","shell.execute_reply.started":"2025-05-13T14:51:35.366462Z","shell.execute_reply":"2025-05-13T14:51:39.676739Z"}},"outputs":[{"name":"stdout","text":"Creating new Ultralytics Settings v0.0.6 file âœ… \nView Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\nUpdate Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/amini-cocoa-contamination-challenge/Train.csv')\nunique_classes = df['class'].unique()\nclass_mapping = {cls: idx for idx, cls in enumerate(unique_classes)}\nprint(class_mapping)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T14:51:39.677897Z","iopub.execute_input":"2025-05-13T14:51:39.678168Z","iopub.status.idle":"2025-05-13T14:51:39.736395Z","shell.execute_reply.started":"2025-05-13T14:51:39.678152Z","shell.execute_reply":"2025-05-13T14:51:39.735665Z"}},"outputs":[{"name":"stdout","text":"{'healthy': 0, 'anthracnose': 1, 'cssvd': 2}\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Set the data directory\nDATA_DIR = Path('/kaggle/input/amini-cocoa-contamination-challenge/')\nIMGS_DIR = Path('/kaggle/input/amini-cocoa-contamination-challenge/dataset/images')\n\n# Load train and test files\ntrain = pd.read_csv(DATA_DIR / 'Train.csv')\ntest = pd.read_csv(DATA_DIR / 'Test.csv')\nss = pd.read_csv(DATA_DIR / 'SampleSubmission.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T14:51:39.738098Z","iopub.execute_input":"2025-05-13T14:51:39.738339Z","iopub.status.idle":"2025-05-13T14:51:42.728439Z","shell.execute_reply.started":"2025-05-13T14:51:39.738323Z","shell.execute_reply":"2025-05-13T14:51:42.727873Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T14:51:42.729101Z","iopub.execute_input":"2025-05-13T14:51:42.729340Z","iopub.status.idle":"2025-05-13T14:51:42.755194Z","shell.execute_reply.started":"2025-05-13T14:51:42.729323Z","shell.execute_reply":"2025-05-13T14:51:42.754729Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"            Image_ID        class  confidence   ymin    xmin    ymax    xmax  \\\n0      ID_nBgcAR.jpg      healthy         1.0   75.0    15.0   162.0   195.0   \n1      ID_nBgcAR.jpg      healthy         1.0   58.0     1.0   133.0   171.0   \n2      ID_nBgcAR.jpg      healthy         1.0   42.0    29.0   377.0   349.0   \n3      ID_Kw2v8A.jpg      healthy         1.0  112.0   124.0   404.0   341.0   \n4      ID_Kw2v8A.jpg      healthy         1.0  148.0   259.0   413.0   412.0   \n...              ...          ...         ...    ...     ...     ...     ...   \n9787  ID_WULBQy.jpeg  anthracnose         1.0  136.0  1440.0  1593.0  4000.0   \n9788  ID_WULBQy.jpeg  anthracnose         1.0   89.0     0.0  1174.0  1139.0   \n9789  ID_SVzl5X.jpeg  anthracnose         1.0   18.0   360.0  1800.0  2330.0   \n9790  ID_xDTIEp.jpeg  anthracnose         1.0  736.0   174.0  2691.0  4032.0   \n9791  ID_VyjCJQ.jpeg  anthracnose         1.0   57.0  1248.0  2735.0  2323.0   \n\n      class_id                            ImagePath  \n0            2   dataset/images/train/ID_nBgcAR.jpg  \n1            2   dataset/images/train/ID_nBgcAR.jpg  \n2            2   dataset/images/train/ID_nBgcAR.jpg  \n3            2   dataset/images/train/ID_Kw2v8A.jpg  \n4            2   dataset/images/train/ID_Kw2v8A.jpg  \n...        ...                                  ...  \n9787         0  dataset/images/train/ID_WULBQy.jpeg  \n9788         0  dataset/images/train/ID_WULBQy.jpeg  \n9789         0  dataset/images/train/ID_SVzl5X.jpeg  \n9790         0  dataset/images/train/ID_xDTIEp.jpeg  \n9791         0  dataset/images/train/ID_VyjCJQ.jpeg  \n\n[9792 rows x 9 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Image_ID</th>\n      <th>class</th>\n      <th>confidence</th>\n      <th>ymin</th>\n      <th>xmin</th>\n      <th>ymax</th>\n      <th>xmax</th>\n      <th>class_id</th>\n      <th>ImagePath</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ID_nBgcAR.jpg</td>\n      <td>healthy</td>\n      <td>1.0</td>\n      <td>75.0</td>\n      <td>15.0</td>\n      <td>162.0</td>\n      <td>195.0</td>\n      <td>2</td>\n      <td>dataset/images/train/ID_nBgcAR.jpg</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ID_nBgcAR.jpg</td>\n      <td>healthy</td>\n      <td>1.0</td>\n      <td>58.0</td>\n      <td>1.0</td>\n      <td>133.0</td>\n      <td>171.0</td>\n      <td>2</td>\n      <td>dataset/images/train/ID_nBgcAR.jpg</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ID_nBgcAR.jpg</td>\n      <td>healthy</td>\n      <td>1.0</td>\n      <td>42.0</td>\n      <td>29.0</td>\n      <td>377.0</td>\n      <td>349.0</td>\n      <td>2</td>\n      <td>dataset/images/train/ID_nBgcAR.jpg</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ID_Kw2v8A.jpg</td>\n      <td>healthy</td>\n      <td>1.0</td>\n      <td>112.0</td>\n      <td>124.0</td>\n      <td>404.0</td>\n      <td>341.0</td>\n      <td>2</td>\n      <td>dataset/images/train/ID_Kw2v8A.jpg</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ID_Kw2v8A.jpg</td>\n      <td>healthy</td>\n      <td>1.0</td>\n      <td>148.0</td>\n      <td>259.0</td>\n      <td>413.0</td>\n      <td>412.0</td>\n      <td>2</td>\n      <td>dataset/images/train/ID_Kw2v8A.jpg</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9787</th>\n      <td>ID_WULBQy.jpeg</td>\n      <td>anthracnose</td>\n      <td>1.0</td>\n      <td>136.0</td>\n      <td>1440.0</td>\n      <td>1593.0</td>\n      <td>4000.0</td>\n      <td>0</td>\n      <td>dataset/images/train/ID_WULBQy.jpeg</td>\n    </tr>\n    <tr>\n      <th>9788</th>\n      <td>ID_WULBQy.jpeg</td>\n      <td>anthracnose</td>\n      <td>1.0</td>\n      <td>89.0</td>\n      <td>0.0</td>\n      <td>1174.0</td>\n      <td>1139.0</td>\n      <td>0</td>\n      <td>dataset/images/train/ID_WULBQy.jpeg</td>\n    </tr>\n    <tr>\n      <th>9789</th>\n      <td>ID_SVzl5X.jpeg</td>\n      <td>anthracnose</td>\n      <td>1.0</td>\n      <td>18.0</td>\n      <td>360.0</td>\n      <td>1800.0</td>\n      <td>2330.0</td>\n      <td>0</td>\n      <td>dataset/images/train/ID_SVzl5X.jpeg</td>\n    </tr>\n    <tr>\n      <th>9790</th>\n      <td>ID_xDTIEp.jpeg</td>\n      <td>anthracnose</td>\n      <td>1.0</td>\n      <td>736.0</td>\n      <td>174.0</td>\n      <td>2691.0</td>\n      <td>4032.0</td>\n      <td>0</td>\n      <td>dataset/images/train/ID_xDTIEp.jpeg</td>\n    </tr>\n    <tr>\n      <th>9791</th>\n      <td>ID_VyjCJQ.jpeg</td>\n      <td>anthracnose</td>\n      <td>1.0</td>\n      <td>57.0</td>\n      <td>1248.0</td>\n      <td>2735.0</td>\n      <td>2323.0</td>\n      <td>0</td>\n      <td>dataset/images/train/ID_VyjCJQ.jpeg</td>\n    </tr>\n  </tbody>\n</table>\n<p>9792 rows Ã— 9 columns</p>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"defined_cols = ['Image_ID',\t'confidence',\t'class',\t'ymin',\t'xmin',\t'ymax',\t'xmax']\ntrain = train[defined_cols]\nprint(f'Sum of duplicated colums: {train.duplicated().sum()}')\nprint(f'Size of dataframe before removing duplicates: {train.shape}')\n# remove duplicates\ntrain = train.drop_duplicates()\nprint(f'Sum of duplicated colums after removing duplicates: {train.duplicated().sum()}')\nprint(f'Size of dataframe after removing duplicates: {train.shape}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T14:51:42.755814Z","iopub.execute_input":"2025-05-13T14:51:42.756001Z","iopub.status.idle":"2025-05-13T14:51:42.775818Z","shell.execute_reply.started":"2025-05-13T14:51:42.755983Z","shell.execute_reply":"2025-05-13T14:51:42.775290Z"}},"outputs":[{"name":"stdout","text":"Sum of duplicated colums: 0\nSize of dataframe before removing duplicates: (9792, 7)\nSum of duplicated colums after removing duplicates: 0\nSize of dataframe after removing duplicates: (9792, 7)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"unique_classes = train['class'].unique()\nfull_label_dict = {cls: idx for idx, cls in enumerate(unique_classes)}\nfull_label_dict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T14:51:42.776446Z","iopub.execute_input":"2025-05-13T14:51:42.776708Z","iopub.status.idle":"2025-05-13T14:51:42.782103Z","shell.execute_reply.started":"2025-05-13T14:51:42.776689Z","shell.execute_reply":"2025-05-13T14:51:42.781403Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"{'healthy': 0, 'anthracnose': 1, 'cssvd': 2}"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"class CFG:\n    seed = 42\n    random_state = 42\n    folds=10\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    torch.use_deterministic_algorithms(True, warn_only=True)\nseed_everything(CFG.seed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T14:51:42.782768Z","iopub.execute_input":"2025-05-13T14:51:42.782932Z","iopub.status.idle":"2025-05-13T14:51:42.797319Z","shell.execute_reply.started":"2025-05-13T14:51:42.782914Z","shell.execute_reply":"2025-05-13T14:51:42.796658Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Step 1: Group by Image_ID and aggregate class labels into lists\ntrain['new_class'] = train['class'].map(full_label_dict)\ngrouped = train.groupby('Image_ID')['new_class'].apply(list).reset_index()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T14:51:42.797971Z","iopub.execute_input":"2025-05-13T14:51:42.798146Z","iopub.status.idle":"2025-05-13T14:51:42.879156Z","shell.execute_reply.started":"2025-05-13T14:51:42.798132Z","shell.execute_reply":"2025-05-13T14:51:42.878695Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"all_classes = train[\"class\"].unique().tolist()\nfor unique_class in all_classes:\n    grouped[unique_class] = -1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T14:51:42.880897Z","iopub.execute_input":"2025-05-13T14:51:42.881369Z","iopub.status.idle":"2025-05-13T14:51:42.886124Z","shell.execute_reply.started":"2025-05-13T14:51:42.881353Z","shell.execute_reply":"2025-05-13T14:51:42.885572Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"reverse_label_mapping = {full_label_dict[key]:key for key in full_label_dict} ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T14:51:42.886648Z","iopub.execute_input":"2025-05-13T14:51:42.886874Z","iopub.status.idle":"2025-05-13T14:51:42.896191Z","shell.execute_reply.started":"2025-05-13T14:51:42.886859Z","shell.execute_reply":"2025-05-13T14:51:42.895613Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"reverse_label_mapping","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T14:51:42.896805Z","iopub.execute_input":"2025-05-13T14:51:42.897035Z","iopub.status.idle":"2025-05-13T14:51:42.908379Z","shell.execute_reply.started":"2025-05-13T14:51:42.897016Z","shell.execute_reply":"2025-05-13T14:51:42.907778Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"{0: 'healthy', 1: 'anthracnose', 2: 'cssvd'}"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"# input 1 if the label is in that image else 0\nall_labels_list = (list(grouped['new_class'].values))\nfor train_index, label_List in enumerate(all_labels_list):\n    unique_labels = list(set(label_List))\n    for label_index in range(len(unique_labels)):\n        label = int(unique_labels[label_index])\n        for key_value in range(23):\n            if label == key_value:\n                grouped.loc[train_index, reverse_label_mapping[key_value]] = 1\n                break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T14:51:42.909029Z","iopub.execute_input":"2025-05-13T14:51:42.909310Z","iopub.status.idle":"2025-05-13T14:51:43.524524Z","shell.execute_reply.started":"2025-05-13T14:51:42.909288Z","shell.execute_reply":"2025-05-13T14:51:43.523977Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"X = grouped[['Image_ID']]\ngrouped['fold'] = -1\nmskf = MultilabelStratifiedKFold(n_splits=CFG.folds, shuffle=True, random_state=CFG.random_state)\nfor i_fold, (train_index, test_index) in enumerate(mskf.split(X, grouped[all_classes])):\n    grouped.loc[test_index, \"fold\"] = i_fold \n\n\n# create image_path for grouped_data\ngrouped['image_path'] = [Path(str(IMGS_DIR) + '/train/' + x) for x in grouped.Image_ID]\n\n# drop duplicates rows for test\ntest = test.drop_duplicates(subset=['Image_ID'], ignore_index=True)\ntest['image_path'] = [Path(str(IMGS_DIR) + '/test/' + x) for x in test.Image_ID]      ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T14:51:43.525192Z","iopub.execute_input":"2025-05-13T14:51:43.525393Z","iopub.status.idle":"2025-05-13T14:51:43.838506Z","shell.execute_reply.started":"2025-05-13T14:51:43.525378Z","shell.execute_reply":"2025-05-13T14:51:43.838004Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Function to convert the bounding boxes to YOLO format and save them\ndef save_yolo_annotation(row):\n\n    image_path, class_id, output_dir = row['image_path'], row['class_id'], row['output_dir']\n\n    img = cv2.imread(str(image_path))\n    if img is None:\n        raise ValueError(f\"Could not read image from path: {image_path}\")\n\n    height, width, _ = img.shape\n    label_file = Path(output_dir) / f\"{Path(image_path).stem}.txt\"\n\n\n    ymin, xmin, ymax, xmax = row['ymin'], row['xmin'], row['ymax'], row['xmax']\n\n    # Normalize the coordinates\n    x_center = (xmin + xmax) / 2 / width\n    y_center = (ymin + ymax) / 2 / height\n    bbox_width = (xmax - xmin) / width\n    bbox_height = (ymax - ymin) / height\n\n    with open(label_file, 'a') as f:\n        f.write(f\"{class_id} {x_center:.6f} {y_center:.6f} {bbox_width:.6f} {bbox_height:.6f}\\n\")\n\n# Parallelize the annotation saving process\ndef process_dataset(dataframe, output_dir):\n    dataframe['output_dir'] = output_dir\n    # convert the dataframe to a dictionary\n    dataframe = dataframe.to_dict('records')\n    for i in tqdm(range(len(dataframe))):\n        save_yolo_annotation(dataframe[i])\n\n\n    # with multiprocessing.Pool(1) as pool:\n    #     list(tqdm(pool.imap(save_yolo_annotation, dataframe.head().to_dict('records')), total=len(dataframe.head())))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T14:51:43.839351Z","iopub.execute_input":"2025-05-13T14:51:43.839558Z","iopub.status.idle":"2025-05-13T14:51:43.845413Z","shell.execute_reply.started":"2025-05-13T14:51:43.839541Z","shell.execute_reply":"2025-05-13T14:51:43.844771Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# # Add an image_path column\ntrain['image_path'] = [Path(str(IMGS_DIR) + '/train/' + x) for x in train.Image_ID]\n\n# Map string classes to integers (label encoding targets)\ntrain['class_id'] = train['class'].map(full_label_dict)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T14:51:43.846023Z","iopub.execute_input":"2025-05-13T14:51:43.846225Z","iopub.status.idle":"2025-05-13T14:51:43.910134Z","shell.execute_reply.started":"2025-05-13T14:51:43.846206Z","shell.execute_reply":"2025-05-13T14:51:43.909650Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"for fold in range(CFG.folds):\n    if fold == 1:\n        # images\n        TRAIN_IMAGES_DIR = Path(f'/kaggle/working/train/images/fold_{fold + 1}')\n        VAL_IMAGES_DIR = Path(f'/kaggle/working/val/images/fold_{fold + 1}')\n        TEST_IMAGES_DIR = Path('/kaggle/working/test/images')\n\n        # labels\n        TRAIN_LABELS_DIR = Path(f'/kaggle/working/train/labels/fold_{fold + 1}')\n        VAL_LABELS_DIR = Path(f'/kaggle/working/val/labels/fold_{fold + 1}')\n        TEST_LABELS_DIR = Path('/kaggle/working/test/labels')\n\n        # get the train and val for that fold\n        train_fold = grouped[grouped['fold'] != fold ].reset_index(drop=True)\n        val_fold = grouped[grouped['fold'] == fold].reset_index(drop=True)\n\n        DIRS = [TRAIN_IMAGES_DIR, VAL_IMAGES_DIR, TRAIN_LABELS_DIR, VAL_LABELS_DIR, TEST_IMAGES_DIR, TEST_LABELS_DIR]\n        \n        # Create necessary directories\n        for DIR in DIRS:\n            if DIR.exists():\n                shutil.rmtree(DIR)\n            DIR.mkdir(parents=True, exist_ok=True)\n       \n        # Copy train, val, and test images to their respective dirs\n        for img in tqdm(train_fold.image_path.unique()):\n            shutil.copy(img, TRAIN_IMAGES_DIR / img.parts[-1])\n        print(f'Copied train file for fold{fold+1} to folder')\n\n        for img in tqdm(val_fold.image_path.unique()):\n            shutil.copy(img, VAL_IMAGES_DIR / img.parts[-1])\n        print(f'Copied val file for fold{fold+1} to folder')\n\n        for img in tqdm(test.image_path.unique()):\n            shutil.copy(img, TEST_IMAGES_DIR / img.parts[-1])\n        print(f'Copied test file for first fold to folder')\n\n\n        X_train = train[train.Image_ID.isin(train_fold.Image_ID)].reset_index(drop=True)\n        X_val = train[train.Image_ID.isin(val_fold.Image_ID)].reset_index(drop=True)\n\n\n        print(f\"-------------Process Datasets for fold {fold+1}\")\n        # Save train and validation labels to their respective dirs\n        process_dataset(X_train, TRAIN_LABELS_DIR)\n        process_dataset(X_val, VAL_LABELS_DIR)\n\n        print(f\"-------------End of Processing of Datasets for fold {fold+1}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T14:51:43.910821Z","iopub.execute_input":"2025-05-13T14:51:43.911009Z","iopub.status.idle":"2025-05-13T15:03:27.191721Z","shell.execute_reply.started":"2025-05-13T14:51:43.910995Z","shell.execute_reply":"2025-05-13T15:03:27.190893Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4976 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57ed21f7bf774a56931d81a5314df21e"}},"metadata":{}},{"name":"stdout","text":"Copied train file for fold2 to folder\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/553 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f884d2c5177b4b3db211029a332168b1"}},"metadata":{}},{"name":"stdout","text":"Copied val file for fold2 to folder\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1626 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6518ea080f24a8886a800ea2aa38006"}},"metadata":{}},{"name":"stdout","text":"Copied test file for first fold to folder\n-------------Process Datasets for fold 2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/8849 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5f570b6385548a893193e9ec8b581e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/943 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c62a8bb08f0421b90292e6b0bb72213"}},"metadata":{}},{"name":"stdout","text":"-------------End of Processing of Datasets for fold 2\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# Define the new dataset directory structure within the current working directory\nbase_dir = './datasets'  # Create the dataset in the local writable directory\ndirs = [\n    os.path.join(base_dir, 'train/images'),\n    os.path.join(base_dir, 'train/labels'),\n    os.path.join(base_dir, 'val/images'),\n    os.path.join(base_dir, 'val/labels')\n]\n\n# Create the directories\nfor dir_path in dirs:\n    os.makedirs(dir_path, exist_ok=True)\n\n# Example: Source directories where your current files are stored (update these paths)\nsource_train_images = './train/images'\nsource_train_labels = './train/labels'\nsource_val_images = './val/images'\nsource_val_labels = './val/labels'\n\n# Move files to the new structure\ndef move_files(source, destination):\n    if os.path.exists(source):\n        for file_name in os.listdir(source):\n            shutil.move(os.path.join(source, file_name), destination)\n\n# Move training images and labels\nmove_files(source_train_images, os.path.join(base_dir, 'train/images'))\nmove_files(source_train_labels, os.path.join(base_dir, 'train/labels'))\n\n# Move validation images and labels\nmove_files(source_val_images, os.path.join(base_dir, 'val/images'))\nmove_files(source_val_labels, os.path.join(base_dir, 'val/labels'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:03:27.192574Z","iopub.execute_input":"2025-05-13T15:03:27.192847Z","iopub.status.idle":"2025-05-13T15:03:27.199623Z","shell.execute_reply.started":"2025-05-13T15:03:27.192824Z","shell.execute_reply":"2025-05-13T15:03:27.199018Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Create a data.yaml file required by YOLO\nclass_names = train['class'].unique().tolist()\nnum_classes = len(class_names)\n\n\n# for fold in range(CFG.folds):\n# images\nTRAIN_IMAGES_DIR = Path('/kaggle/working/datasets/train/images/fold_2/')\nVAL_IMAGES_DIR = Path('/kaggle/working/datasets/val/images/fold_2/')\n\ndata_yaml = {\n    'train': str(TRAIN_IMAGES_DIR),\n    'val': str(VAL_IMAGES_DIR),\n    'nc': num_classes,\n    'names': class_names\n}\n\n# Save the data.yaml file\nyaml_path = Path('data.yaml')\nwith open(yaml_path, 'w') as file:\n    yaml.dump(data_yaml, file, default_flow_style=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:03:27.200315Z","iopub.execute_input":"2025-05-13T15:03:27.200568Z","iopub.status.idle":"2025-05-13T15:03:27.216069Z","shell.execute_reply.started":"2025-05-13T15:03:27.200545Z","shell.execute_reply":"2025-05-13T15:03:27.215526Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"data_yaml","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:03:27.216791Z","iopub.execute_input":"2025-05-13T15:03:27.216999Z","iopub.status.idle":"2025-05-13T15:03:27.227620Z","shell.execute_reply.started":"2025-05-13T15:03:27.216985Z","shell.execute_reply":"2025-05-13T15:03:27.226958Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"{'train': '/kaggle/working/datasets/train/images/fold_2',\n 'val': '/kaggle/working/datasets/val/images/fold_2',\n 'nc': 3,\n 'names': ['healthy', 'anthracnose', 'cssvd']}"},"metadata":{}}],"execution_count":20},{"cell_type":"markdown","source":"## ğŸ§  Train Second Model, A YOLOv11 Model on Custom YoloWeightedDataset","metadata":{}},{"cell_type":"code","source":"class YOLOWeightedDataset(YOLODataset):\n    def __init__(self, *args, mode=\"train\", **kwargs):\n        \"\"\"\n        Initialize the WeightedDataset.\n\n        Args:\n            class_weights (list or numpy array): A list or array of weights corresponding to each class.\n        \"\"\"\n\n        super(YOLOWeightedDataset, self).__init__(*args, **kwargs)\n\n        self.train_mode = \"train\" in self.prefix\n\n        # You can also specify weights manually instead\n        self.count_instances()\n        class_weights = np.sum(self.counts) / self.counts\n\n        # Aggregation function\n        self.agg_func = np.mean\n\n        self.class_weights = np.array(class_weights)\n        self.weights = self.calculate_weights()\n        self.probabilities = self.calculate_probabilities()\n\n    def count_instances(self):\n        \"\"\"\n        Count the number of instances per class\n\n        Returns:\n            dict: A dict containing the counts for each class.\n        \"\"\"\n        self.counts = [0 for i in range(len(self.data[\"names\"]))]\n        for label in self.labels:\n            cls = label['cls'].reshape(-1).astype(int)\n            for id in cls:\n                self.counts[id] += 1\n\n        self.counts = np.array(self.counts)\n        self.counts = np.where(self.counts == 0, 1, self.counts)\n\n    def calculate_weights(self):\n        \"\"\"\n        Calculate the aggregated weight for each label based on class weights.\n\n        Returns:\n            list: A list of aggregated weights corresponding to each label.\n        \"\"\"\n        weights = []\n        for label in self.labels:\n            cls = label['cls'].reshape(-1).astype(int)\n\n            # Give a default weight to background class\n            if cls.size == 0:\n              weights.append(1)\n              continue\n\n            # Take mean of weights\n            # You can change this weight aggregation function to aggregate weights differently\n            weight = self.agg_func(self.class_weights[cls])\n            weights.append(weight)\n        return weights\n\n    def calculate_probabilities(self):\n        \"\"\"\n        Calculate and store the sampling probabilities based on the weights.\n\n        Returns:\n            list: A list of sampling probabilities corresponding to each label.\n        \"\"\"\n        total_weight = sum(self.weights)\n        probabilities = [w / total_weight for w in self.weights]\n        return probabilities\n\n    def __getitem__(self, index):\n        \"\"\"\n        Return transformed label information based on the sampled index.\n        \"\"\"\n        # Don't use for validation\n        if not self.train_mode:\n            return self.transforms(self.get_image_and_label(index))\n        else:\n            index = np.random.choice(len(self.labels), p=self.probabilities)\n            return self.transforms(self.get_image_and_label(index))\n\nbuild.YOLODataset = YOLOWeightedDataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model2 = YOLO(\"yolo11l.pt\")\nmodel2.train(data='data.yaml',\n                      epochs=100,\n                      imgsz=640,\n                      device=0,\n                      batch=16,\n                      optimizer='AdamW',\n                      lr0=3e-4,\n                      momentum=0.9,\n                      weight_decay=1e-2,\n                      close_mosaic=30,\n                      seed=42,\n                      patience=10\n           )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results = model2.val()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T15:03:27.229725Z","iopub.execute_input":"2025-05-13T15:03:27.229904Z","iopub.status.idle":"2025-05-13T15:03:58.219827Z","shell.execute_reply.started":"2025-05-13T15:03:27.229891Z","shell.execute_reply":"2025-05-13T15:03:58.218848Z"}},"outputs":[{"name":"stdout","text":"Ultralytics 8.3.133 ğŸš€ Python-3.11.11 torch-2.5.1+cu124 CUDA:0 (Tesla T4, 15095MiB)\nYOLO11l summary (fused): 190 layers, 25,281,625 parameters, 0 gradients, 86.6 GFLOPs\nDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 755k/755k [00:00<00:00, 16.3MB/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 2868.3Â±1620.4 MB/s, size: 1661.7 KB)\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/working/datasets/val/labels/fold_2... 553 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 553/553 [00:03<00:00, 178.50it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_BFveJq.jpeg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_By57N4.jpeg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_EJWqGf.jpeg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_EUJ6CX.jpeg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_FtYRqz.jpeg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_HuMwmi.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_J7hL2Y.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_NGiOVN.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_SuY5t1.jpeg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_YevJzo.jpeg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_Zwe8i7.jpeg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_bCoAZg.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_bouUUL.jpeg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_f22xL0.jpeg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_jbDUGG.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_jkpHUf.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_k7E9yz.jpeg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_krz5Kc.jpeg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_mhIu8M.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_oOXqz9.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_q0MS9f.jpeg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_sZNOcT.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_soB3nB.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_teXsR2.jpeg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_uKHtOl.jpeg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_uiDeaI.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_unaCV7.jpeg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_y31JbP.jpeg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_yol9H8.jpeg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_z6gC1L.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /kaggle/working/datasets/val/labels/fold_2.cache\n","output_type":"stream"},{"name":"stderr","text":"\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:17<00:00,  1.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        553        943      0.791      0.736      0.818      0.571\n               healthy        172        386      0.768      0.725      0.808      0.551\n           anthracnose        136        194      0.785      0.784      0.852      0.613\n                 cssvd        246        363      0.819        0.7      0.793      0.548\nSpeed: 0.2ms preprocess, 22.2ms inference, 0.0ms loss, 1.6ms postprocess per image\nResults saved to \u001b[1mruns/detect/val\u001b[0m\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"Our Cross Validation is at 0.818 which is slightly lower than the first model but is great. Let's now infer the models","metadata":{}},{"cell_type":"markdown","source":"We are so sorry we could not show the train logs for the code review. We were short on time. The training time is slightly below the 9 hour training time for both models.","metadata":{}}]}