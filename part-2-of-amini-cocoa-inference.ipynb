{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10751899,"sourceType":"datasetVersion","datasetId":6668519},{"sourceId":359296,"sourceType":"modelInstanceVersion","modelInstanceId":299187,"modelId":319779},{"sourceId":371816,"sourceType":"modelInstanceVersion","modelInstanceId":307760,"modelId":328214},{"sourceId":307845,"sourceType":"modelInstanceVersion","modelInstanceId":262217,"modelId":283341}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"accelerator":"GPU","papermill":{"default_parameters":{},"duration":9473.623383,"end_time":"2025-05-13T10:07:53.830942","environment_variables":{},"exception":true,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-05-13T07:30:00.207559","version":"2.6.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"481bb8e7","cell_type":"markdown","source":"# Part 2 of Amini Cocoa Contamination Challenge Solution By SPECIALZ🔥🔥🙌🏽 ","metadata":{"papermill":{"duration":0.004977,"end_time":"2025-05-13T07:30:04.473736","exception":false,"start_time":"2025-05-13T07:30:04.468759","status":"completed"},"tags":[]}},{"id":"c0f16961","cell_type":"markdown","source":"## 📈 Inference","metadata":{"id":"BPXJ_eQWq0wY","papermill":{"duration":0.003834,"end_time":"2025-05-13T07:30:04.482093","exception":false,"start_time":"2025-05-13T07:30:04.478259","status":"completed"},"tags":[]}},{"id":"79f5488c","cell_type":"markdown","source":"### Install Necessary Packages","metadata":{"papermill":{"duration":0.003739,"end_time":"2025-05-13T07:30:04.489686","exception":false,"start_time":"2025-05-13T07:30:04.485947","status":"completed"},"tags":[]}},{"id":"b94ec7f2","cell_type":"code","source":"!pip -q install -U ultralytics","metadata":{"execution":{"iopub.execute_input":"2025-05-13T07:30:04.498126Z","iopub.status.busy":"2025-05-13T07:30:04.497928Z","iopub.status.idle":"2025-05-13T07:31:20.767203Z","shell.execute_reply":"2025-05-13T07:31:20.766104Z"},"papermill":{"duration":76.275468,"end_time":"2025-05-13T07:31:20.768948","exception":false,"start_time":"2025-05-13T07:30:04.493480","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m90.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n","pylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\r\n","pylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\r\n","\u001b[0m"]}],"execution_count":1},{"id":"3aee2c15","cell_type":"code","source":"!pip -q install ensemble_boxes","metadata":{"execution":{"iopub.execute_input":"2025-05-13T07:31:20.822078Z","iopub.status.busy":"2025-05-13T07:31:20.821805Z","iopub.status.idle":"2025-05-13T07:31:23.971943Z","shell.execute_reply":"2025-05-13T07:31:23.971124Z"},"papermill":{"duration":3.177579,"end_time":"2025-05-13T07:31:23.973376","exception":false,"start_time":"2025-05-13T07:31:20.795797","status":"completed"},"tags":[]},"outputs":[],"execution_count":2},{"id":"f4869208","cell_type":"markdown","source":"### Import Necessary Packages","metadata":{"papermill":{"duration":0.021409,"end_time":"2025-05-13T07:31:24.016985","exception":false,"start_time":"2025-05-13T07:31:23.995576","status":"completed"},"tags":[]}},{"id":"6df26dd0","cell_type":"code","source":"from collections import defaultdict\nimport pandas as pd, numpy as np\nimport sys,os,shutil,gc,re,json,glob,math,time,random,warnings\nfrom tqdm import tqdm\nfrom sklearn.model_selection import StratifiedGroupKFold\nimport sklearn.metrics as skm\nimport torch\nimport cv2\nimport yaml\nimport albumentations as A\nfrom ultralytics.data.build import YOLODataset\nimport ultralytics.data.build as build\nfrom ultralytics import YOLO\nfrom ensemble_boxes import *","metadata":{"execution":{"iopub.execute_input":"2025-05-13T07:31:24.061188Z","iopub.status.busy":"2025-05-13T07:31:24.060951Z","iopub.status.idle":"2025-05-13T07:31:33.231154Z","shell.execute_reply":"2025-05-13T07:31:33.230595Z"},"id":"VXdsJ8nq3vX5","outputId":"267ec1e7-1674-44a2-e7cf-88a0317b30ce","papermill":{"duration":9.194051,"end_time":"2025-05-13T07:31:33.232465","exception":false,"start_time":"2025-05-13T07:31:24.038414","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Creating new Ultralytics Settings v0.0.6 file ✅ \n","View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n","Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/albumentations/__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.6' (you have '2.0.4'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n","  check_for_updates()\n"]}],"execution_count":3},{"id":"84ccfd2a","cell_type":"markdown","source":"### 📁  Set configurations and seed","metadata":{"papermill":{"duration":0.021527,"end_time":"2025-05-13T07:31:33.276428","exception":false,"start_time":"2025-05-13T07:31:33.254901","status":"completed"},"tags":[]}},{"id":"fcc6dbd2","cell_type":"code","source":"N_SPLITS = 5\nRANDOM_STATE = 42\nFOLD=0\n\ndef fix_seed(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.benchmark = False\n\nfix_seed(RANDOM_STATE)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.execute_input":"2025-05-13T07:31:33.320785Z","iopub.status.busy":"2025-05-13T07:31:33.320077Z","iopub.status.idle":"2025-05-13T07:31:33.413458Z","shell.execute_reply":"2025-05-13T07:31:33.412697Z"},"papermill":{"duration":0.116962,"end_time":"2025-05-13T07:31:33.414804","exception":false,"start_time":"2025-05-13T07:31:33.297842","status":"completed"},"tags":[]},"outputs":[],"execution_count":4},{"id":"1df85c5d","cell_type":"markdown","source":"### 📁 Directory and Data Loading","metadata":{"papermill":{"duration":0.021431,"end_time":"2025-05-13T07:31:33.458527","exception":false,"start_time":"2025-05-13T07:31:33.437096","status":"completed"},"tags":[]}},{"id":"825e82b9","cell_type":"code","source":"DIR_DATA = '/kaggle/input/amini-cocoa-contamination-challenge'\ndf_test = pd.read_csv(f'{DIR_DATA}/Test.csv')\ndf_test['path'] = f'{DIR_DATA}/dataset/images/test/'+df_test.Image_ID\ndf_test.shape","metadata":{"execution":{"iopub.execute_input":"2025-05-13T07:31:33.547046Z","iopub.status.busy":"2025-05-13T07:31:33.546366Z","iopub.status.idle":"2025-05-13T07:31:33.579688Z","shell.execute_reply":"2025-05-13T07:31:33.578997Z"},"id":"3eQF2Rut5eKL","outputId":"30ac84a7-336a-4a21-9739-586001766101","papermill":{"duration":0.05832,"end_time":"2025-05-13T07:31:33.580864","exception":false,"start_time":"2025-05-13T07:31:33.522544","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["(1626, 10)"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"execution_count":5},{"id":"c5301c63","cell_type":"code","source":"assert len(df_test) == df_test.Image_ID.nunique()","metadata":{"execution":{"iopub.execute_input":"2025-05-13T07:31:33.625812Z","iopub.status.busy":"2025-05-13T07:31:33.625604Z","iopub.status.idle":"2025-05-13T07:31:33.632445Z","shell.execute_reply":"2025-05-13T07:31:33.631702Z"},"id":"aaEkERgyRfWx","papermill":{"duration":0.030382,"end_time":"2025-05-13T07:31:33.633523","exception":false,"start_time":"2025-05-13T07:31:33.603141","status":"completed"},"tags":[]},"outputs":[],"execution_count":6},{"id":"123cf025","cell_type":"markdown","source":"### ✅ Explanation\nThis line checks that every row in the df_test DataFrame has a unique Image_ID.\n\n<code>len(df_test)</code> gives the total number of rows.\n\n<code>df_test.Image_ID.nunique()</code> counts the number of unique Image_ID values.\n\nIf these two numbers are not equal, the assertion will raise an AssertionError, indicating there are duplicate image entries in the test set — which may lead to data leakage or evaluation issues.","metadata":{"papermill":{"duration":0.021638,"end_time":"2025-05-13T07:31:33.676832","exception":false,"start_time":"2025-05-13T07:31:33.655194","status":"completed"},"tags":[]}},{"id":"975b1dad","cell_type":"code","source":"# List of image sizes to infer on\nIMGSZS =  [288,384,448,480,544,576,608,640,672,736,768,800,832,864,896]\nlen(IMGSZS)","metadata":{"execution":{"iopub.execute_input":"2025-05-13T07:31:33.722243Z","iopub.status.busy":"2025-05-13T07:31:33.721628Z","iopub.status.idle":"2025-05-13T07:31:33.726338Z","shell.execute_reply":"2025-05-13T07:31:33.725808Z"},"id":"uTLU2OtWnYYa","outputId":"4b730879-f865-4876-929f-c43e7fe54425","papermill":{"duration":0.028597,"end_time":"2025-05-13T07:31:33.727366","exception":false,"start_time":"2025-05-13T07:31:33.698769","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["15"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"execution_count":7},{"id":"6168a2f3","cell_type":"markdown","source":"### 🧠 Explanation\nIMGSZS is a list of different image resolutions that will be used for inference.\n\nThese varying sizes can help evaluate the model's robustness across different input scales — a technique often used in test-time augmentation or ensembling.\n\n<code>len(IMGSZS)</code> returns the total number of image sizes in the list, which is 15 in this case.\n\n","metadata":{"papermill":{"duration":0.021417,"end_time":"2025-05-13T07:31:33.770553","exception":false,"start_time":"2025-05-13T07:31:33.749136","status":"completed"},"tags":[]}},{"id":"1b3c7d22","cell_type":"code","source":"# Define mappings between class names and their corresponding IDs\nclass2id = {'healthy': 0, 'anthracnose': 1, 'cssvd': 2}\nid2class = {v:k for k,v in class2id.items()}\nprint(id2class)","metadata":{"execution":{"iopub.execute_input":"2025-05-13T07:31:33.814392Z","iopub.status.busy":"2025-05-13T07:31:33.814161Z","iopub.status.idle":"2025-05-13T07:31:33.817998Z","shell.execute_reply":"2025-05-13T07:31:33.817358Z"},"id":"i32lvWtDPlkw","outputId":"36a2aba5-e87e-47a3-fdeb-04076be7dd82","papermill":{"duration":0.026994,"end_time":"2025-05-13T07:31:33.819016","exception":false,"start_time":"2025-05-13T07:31:33.792022","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["{0: 'healthy', 1: 'anthracnose', 2: 'cssvd'}\n"]}],"execution_count":8},{"id":"61b33c14","cell_type":"markdown","source":"### 🧠 Explanation\n<code>class2id</code> is a dictionary that maps class names to numeric IDs.\n\n<code>id2class</code>  is the reverse mapping, converting numeric IDs back to class names. This is useful for interpreting predictions.","metadata":{"papermill":{"duration":0.021669,"end_time":"2025-05-13T07:31:33.864077","exception":false,"start_time":"2025-05-13T07:31:33.842408","status":"completed"},"tags":[]}},{"id":"83fb17a6","cell_type":"markdown","source":"## 🔍 Function: run_model_on_test\nThis function runs a trained YOLO model on a test dataset across multiple image sizes.\n\n### 📥 Inputs:\n<code>df_test</code>: A DataFrame containing test image paths and IDs.\n\n<code>model_path</code>: Path to the trained YOLO model weights.\n\n<code>all_data</code>: A dictionary to store detection results for different image sizes.\n\n### ⚙️ Steps:\n1. Loads the YOLO model in evaluation mode.\n\n2. Loops through each test image and infers using multiple resolutions defined in IMGSZS.\n\nFor each image size:\n\n1. Runs the model with specified parameters (e.g., augment=True, iou=0.5).\n\n2. Extracts bounding boxes, classes, and confidence scores.\n\n3. Applies a confidence threshold (minconf).\n\n4. If no objects are detected, defaults to 'healthy' class with zero bounding box.\n\n5. Otherwise, logs each detection (class, confidence, bounding box coordinates).\n\n6. Appends the results to all_data under the corresponding image size.\n\n### 📤 Output:\nReturns all_data, populated with detection results for each image and size.\n\n","metadata":{"papermill":{"duration":0.021522,"end_time":"2025-05-13T07:31:33.907599","exception":false,"start_time":"2025-05-13T07:31:33.886077","status":"completed"},"tags":[]}},{"id":"681c5627","cell_type":"code","source":"def run_model_on_test(df_test, model_path, all_data):\n    model = YOLO(model_path, task='detect')\n    model.eval()\n    model.training = False\n    minconf = 0.0\n\n    for _, row in tqdm(df_test.iterrows(), total=len(df_test)):\n        img = cv2.imread(row.path)\n        h, w, _ = img.shape\n\n        for imgsz in IMGSZS:\n            results = model(img, imgsz=imgsz, conf=minconf, augment=True, iou=0.5, max_det=1500, verbose=False)[0]\n            boxes = results.boxes.xyxy.cpu().numpy()\n            classes = results.boxes.cls.cpu().numpy()\n            confidences = results.boxes.conf.cpu().numpy()\n\n            ixs = confidences >= minconf\n            boxes = boxes[ixs]\n            classes = classes[ixs]\n            confidences = confidences[ixs]\n\n            if len(boxes) == 0:\n                all_data[imgsz].append({\n                    'Image_ID': row.Image_ID, 'class': 'healthy', 'confidence': 0,\n                    'ymin': 0, 'xmin': 0, 'ymax': 0, 'xmax': 0\n                })\n            else:\n                for box, cls, conf in zip(boxes, classes, confidences):\n                    x1, y1, x2, y2 = box\n                    detected_class = id2class[int(cls)]\n                    all_data[imgsz].append({\n                        'Image_ID': row.Image_ID, 'class': detected_class, 'confidence': conf,\n                        'ymin': y1, 'xmin': x1, 'ymax': y2, 'xmax': x2\n                    })\n\n    return all_data","metadata":{"execution":{"iopub.execute_input":"2025-05-13T07:31:33.951790Z","iopub.status.busy":"2025-05-13T07:31:33.951605Z","iopub.status.idle":"2025-05-13T07:31:33.958095Z","shell.execute_reply":"2025-05-13T07:31:33.957613Z"},"id":"4R3l7oX7T44n","outputId":"f601a796-8dc3-4c44-c109-e66a6567bcb4","papermill":{"duration":0.029722,"end_time":"2025-05-13T07:31:33.959093","exception":false,"start_time":"2025-05-13T07:31:33.929371","status":"completed"},"tags":[]},"outputs":[],"execution_count":9},{"id":"9dfb50f9","cell_type":"markdown","source":"## 🧪 Inference on Test Set with Multiple Models\nThis code runs inference on the test set using two trained YOLO models (model1 and model2) and collects predictions for further analysis or ensembling.\n\n#### 🗂 Steps:\n<code>defaultdict(list)</code> initializes a dictionary where each value is an empty list by default, useful for storing results grouped by image sizes (IMGSZS).\n\n<code>run_model_on_test(...)</code> runs each model on the test DataFrame df_test, storing predictions like bounding boxes, classes, and confidence scores into the corresponding dictionary (all_data_model1 or all_data_model2).\n\nEach dictionary will contain structured detection outputs per image size, useful for downstream tasks like post-processing, majority voting, or performance comparison between models.","metadata":{"papermill":{"duration":0.021699,"end_time":"2025-05-13T07:31:34.002753","exception":false,"start_time":"2025-05-13T07:31:33.981054","status":"completed"},"tags":[]}},{"id":"a884a956","cell_type":"markdown","source":"Make sure to take the right models from the model development notebook\n\nI named ours:\n```yaml\n1. Model with 0.821 CV: '/kaggle/input/amini-cocoa-0.821-model/pytorch/default/1/best.pt'\n2. Model with 0.818 CV: '/kaggle/input/yolo11l-weighteddataset-640imgsz-100epochs/pytorch/default/1/yolo11_balanced_best.pt'\n```","metadata":{"papermill":{"duration":0.021744,"end_time":"2025-05-13T07:31:34.046524","exception":false,"start_time":"2025-05-13T07:31:34.024780","status":"completed"},"tags":[]}},{"id":"386a70ae","cell_type":"code","source":"all_data_model1 = defaultdict(list)\nall_data_model2 = defaultdict(list)\n\nall_data_model1 = run_model_on_test(df_test, '/kaggle/input/amini-cocoa-0.821-model/pytorch/default/1/best.pt', all_data_model1)\nall_data_model2 = run_model_on_test(df_test, '/kaggle/input/yolo11l-weighteddataset-640imgsz-100epochs/pytorch/default/1/yolo11_balanced_best.pt', all_data_model2)\n","metadata":{"execution":{"iopub.execute_input":"2025-05-13T07:31:34.092037Z","iopub.status.busy":"2025-05-13T07:31:34.091857Z","iopub.status.idle":"2025-05-13T08:49:54.927423Z","shell.execute_reply":"2025-05-13T08:49:54.926514Z"},"papermill":{"duration":4700.86032,"end_time":"2025-05-13T08:49:54.928735","exception":false,"start_time":"2025-05-13T07:31:34.068415","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 1626/1626 [39:34<00:00,  1.46s/it]\n","100%|██████████| 1626/1626 [38:44<00:00,  1.43s/it]\n"]}],"execution_count":10},{"id":"8f8a9c0c","cell_type":"code","source":"!mkdir -p test_preds\nfor imgsz in IMGSZS:\n    preds1 = pd.DataFrame(all_data_model1[imgsz])\n    preds2 = pd.DataFrame(all_data_model2[imgsz])\n    preds1.to_csv(f'test_preds/preds_model1_{FOLD}_{imgsz}.csv', index=False)\n    preds2.to_csv(f'test_preds/preds_model2_{FOLD}_{imgsz}.csv', index=False)\n    print(f\"{imgsz}: model1={preds1.shape}, model2={preds2.shape}\")","metadata":{"execution":{"iopub.execute_input":"2025-05-13T08:49:55.221989Z","iopub.status.busy":"2025-05-13T08:49:55.221238Z","iopub.status.idle":"2025-05-13T08:54:10.118716Z","shell.execute_reply":"2025-05-13T08:54:10.117765Z"},"id":"1aQSBNUQme09","outputId":"d66f9a3a-430c-4d5b-f441-fccbb6a247cd","papermill":{"duration":255.195602,"end_time":"2025-05-13T08:54:10.271000","exception":false,"start_time":"2025-05-13T08:49:55.075398","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["288: model1=(462869, 7), model2=(439757, 7)\n","384: model1=(562999, 7), model2=(544198, 7)\n","448: model1=(852248, 7), model2=(851033, 7)\n","480: model1=(929564, 7), model2=(915605, 7)\n","544: model1=(977838, 7), model2=(1007606, 7)\n","576: model1=(1080439, 7), model2=(1090088, 7)\n","608: model1=(1227250, 7), model2=(1235520, 7)\n","640: model1=(1124869, 7), model2=(1125167, 7)\n","672: model1=(1331880, 7), model2=(1338767, 7)\n","736: model1=(1609241, 7), model2=(1625663, 7)\n","768: model1=(1357174, 7), model2=(1388915, 7)\n","800: model1=(1560960, 7), model2=(1590612, 7)\n","832: model1=(1785374, 7), model2=(1799329, 7)\n","864: model1=(2002817, 7), model2=(1990378, 7)\n","896: model1=(1816360, 7), model2=(1852289, 7)\n"]}],"execution_count":11},{"id":"61152f48","cell_type":"markdown","source":"## 📦 Save YOLO Test Predictions for Each Image Size\nThis snippet above does the following:\n\nCreates a test_preds directory (if it doesn't already exist).\n\nIterates over all defined image sizes (IMGSZS).\n\nConverts predictions stored in all_data_model1 and all_data_model2 to DataFrames.\n\nSaves each DataFrame as a CSV file named with the model, fold, and image size.\n\nPrints the shape of each prediction file for confirmation.\n\nThis step is useful for debugging and later evaluation or ensembling of predictions across different image sizes and models.\n\n","metadata":{"papermill":{"duration":0.145713,"end_time":"2025-05-13T08:54:10.564391","exception":false,"start_time":"2025-05-13T08:54:10.418678","status":"completed"},"tags":[]}},{"id":"1f1ca171","cell_type":"markdown","source":"## 📦 Function: merge_bboxes\nThis function fuses bounding box predictions from multiple models using Weighted Boxes Fusion (WBF).\n\n### Parameters:\n<code>dfs</code>: List of prediction DataFrames from different models.\n\n<code>iou_thr</code>: IoU threshold for WBF.\n\n<code>skip_box_thr</code>: Confidence threshold to skip low-confidence boxes.\n\n<code>df_meta</code> (optional): Placeholder for additional metadata.\n\n### Steps:\nCollects all unique Image_IDs.\n\nFor each image, collects and normalizes bounding boxes, scores, and class labels from all models.\n\nApplies WBF to merge overlapping predictions.\n\nDenormalizes and reconstructs final bounding boxes.\n\nReturns a unified DataFrame of predictions for all test images.\n\nThis method improves final prediction quality by combining multiple model outputs instead of relying on one.","metadata":{"papermill":{"duration":0.147633,"end_time":"2025-05-13T08:54:10.856624","exception":false,"start_time":"2025-05-13T08:54:10.708991","status":"completed"},"tags":[]}},{"id":"1a02470b","cell_type":"code","source":"def merge_bboxes(dfs,iou_thr,skip_box_thr,df_meta=None):\n    wb_boxes = []; wb_scores = []; wb_labels = []\n    df_res_wbf = []\n    image_ids = pd.concat(dfs).Image_ID.unique().tolist()\n    weights = None\n\n    cols = ['xmin','ymin','xmax','ymax']\n\n    for Image_ID in tqdm(image_ids):\n      boxes_list = []; scores_list = []; labels_list = []\n      h,w,_ = cv2.imread(f'{DIR_DATA}/dataset/images/test/{Image_ID}').shape\n      for ix_res, df_res in enumerate(dfs):\n          d = df_res[df_res.Image_ID==Image_ID].copy()\n\n          boxes = d[cols].values\n          scores = d.confidence.tolist()\n          labels = d['class'].map(class2id).values.tolist()\n\n          boxes_ = []\n          scores_ = []\n          labels_ = []\n          for i,box in enumerate(boxes):\n            if box[2]> box[0] and box[3]>box[1]:\n              boxes_.append(box)\n              scores_.append(scores[i])\n              labels_.append(labels[i])\n\n          boxes = boxes_\n          scores = scores_\n          labels = labels_\n\n          boxes = [[x[0]/w,x[1]/h,x[2]/w,x[3]/h] for x in boxes]\n\n          boxes_list.append(boxes)\n          scores_list.append(scores)\n          labels_list.append(labels)\n\n      if len(boxes_list)>0:\n        boxes, scores, labels = weighted_boxes_fusion(boxes_list, scores_list, labels_list, weights=[1 for i in range(len(boxes_list))], iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n\n      else:\n        boxes = []\n\n      boxes = [[x[0]*w,x[1]*h,x[2]*w,x[3]*h] for x in boxes]\n      ws = [b[2]-b[0] for b in boxes]\n      hs = [b[3]-b[1] for b in boxes]\n\n      if len(boxes)==0:\n        d_res = pd.DataFrame([{ 'Image_ID': Image_ID, 'class': 'healthy', 'confidence': 0.5, 'ymin': 0, 'xmin': 0, 'ymax': 0, 'xmax': 0 }])\n      else:\n        xmin = np.array(boxes)[:,0]\n        ymin = np.array(boxes)[:,1]\n        xmax = np.array(boxes)[:,2]\n        ymax = np.array(boxes)[:,3]\n\n        d_res = pd.DataFrame(dict(Image_ID=Image_ID,confidence=scores,ymin=ymin,xmin=xmin,ymax=ymax,xmax=xmax))\n        d_res['class'] =  [id2class[l] for l in labels ]\n\n      df_res_wbf.append(d_res)\n\n\n    df_res_wbf = pd.concat(df_res_wbf)\n    return df_res_wbf","metadata":{"execution":{"iopub.execute_input":"2025-05-13T08:54:11.211349Z","iopub.status.busy":"2025-05-13T08:54:11.210576Z","iopub.status.idle":"2025-05-13T08:54:11.221813Z","shell.execute_reply":"2025-05-13T08:54:11.221082Z"},"id":"8fNtMvh9TXeY","papermill":{"duration":0.160565,"end_time":"2025-05-13T08:54:11.222937","exception":false,"start_time":"2025-05-13T08:54:11.062372","status":"completed"},"tags":[]},"outputs":[],"execution_count":12},{"id":"621233f8","cell_type":"markdown","source":"## 🧪 Ensembling Predictions with WBF\nThis block:\n\nLoads prediction CSVs for different image sizes from two YOLO models.\n\nAggregates them into a single list of DataFrames.\n\nMerges the predictions using Weighted Boxes Fusion to enhance final detection accuracy.\n\nOutputs the final ensembled predictions into <code>submission.csv</code> for evaluation or submission.\n\nThis method leverages diversity across scales and models to produce more robust results.","metadata":{"papermill":{"duration":0.148088,"end_time":"2025-05-13T08:54:11.519828","exception":false,"start_time":"2025-05-13T08:54:11.371740","status":"completed"},"tags":[]}},{"id":"fd1fba5c","cell_type":"code","source":"dfs = []\nfor imgsz in IMGSZS:\n    df1 = pd.read_csv(f'test_preds/preds_model1_{FOLD}_{imgsz}.csv')\n    df2 = pd.read_csv(f'test_preds/preds_model2_{FOLD}_{imgsz}.csv')\n    dfs.extend([df1, df2])\n\niou_thr = 0.558\nskip_box_thr = 0.005\n\ndf_res_wbf = merge_bboxes(dfs, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\ndf_res_wbf.to_csv('submission.csv', index=False)\nprint(\"Saved to submission.csv\")","metadata":{"execution":{"iopub.execute_input":"2025-05-13T08:54:11.817190Z","iopub.status.busy":"2025-05-13T08:54:11.816898Z","iopub.status.idle":"2025-05-13T10:07:41.887873Z","shell.execute_reply":"2025-05-13T10:07:41.886812Z"},"id":"55rF_dP1s2sq","outputId":"99aa1783-cf7c-481c-f0d7-d7831273577d","papermill":{"duration":4410.219572,"end_time":"2025-05-13T10:07:41.889107","exception":false,"start_time":"2025-05-13T08:54:11.669535","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 1626/1626 [1:12:53<00:00,  2.69s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Saved to submission.csv\n"]}],"execution_count":13},{"id":"31b7e6e4","cell_type":"markdown","source":"### And Don't Just Have Good Day, Have A Great Day!\n## Thank You!","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]}}]}