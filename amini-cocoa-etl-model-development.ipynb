{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10751899,"sourceType":"datasetVersion","datasetId":6668519}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Amini Cocoa Contamination Challenge Solution By SPECIALZüî•üî•üôåüèΩ","metadata":{}},{"cell_type":"markdown","source":"This notebook documents my submission to the Amini Cocoa Contamination Challenge, which aims to build machine learning models capable of identifying multiple cocoa leaf diseases‚Äîlike CSSVD and anthracnose‚Äîdirectly from images. The goal is to develop models that not only generalize well to unseen diseases, but also run efficiently on low-end smartphones, making real-time diagnosis accessible to smallholder farmers across Africa.","metadata":{}},{"cell_type":"markdown","source":"### ‚öôÔ∏è Our Approach\nThroughout this challenge, We experienced several ups and downs‚Äîexperimenting with different YOLO models, adjusting image sizes, tuning confidence thresholds, and merging predictions. Getting a model that balanced accuracy, robustness, and efficiency was not easy. At one point, results improved significantly around the 46th epoch, showing the importance of careful monitoring and validation.\n\nThis notebook covers:\n\nModel training using YOLOv11\n\nInference with test-time augmentation (TTA)\n\nPrediction fusion using Weighted Box Fusion (WBF)\n\nFinal submission preparation\n\nAnd an explainability section to meet the competition requirements\n\nLet‚Äôs dive into the code and explore what worked‚Äîand what didn‚Äôt‚Äîon the path to building an AI solution that could one day live on a farmer‚Äôs phone.","metadata":{}},{"cell_type":"markdown","source":"### Install Necessary Packages","metadata":{}},{"cell_type":"code","source":"!pip -q install -U ultralytics iterative-stratification","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-13T09:11:23.218307Z","iopub.execute_input":"2025-05-13T09:11:23.218847Z","iopub.status.idle":"2025-05-13T09:12:23.442700Z","shell.execute_reply.started":"2025-05-13T09:11:23.218824Z","shell.execute_reply":"2025-05-13T09:12:23.441990Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"### Import Necessary Packages","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport os\nfrom pathlib import Path\nimport shutil\nfrom sklearn.model_selection import train_test_split\nfrom tqdm.notebook import tqdm\nimport cv2\nimport yaml\nimport matplotlib.pyplot as plt\nfrom ultralytics import YOLO\nimport multiprocessing\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport random\nfrom datetime import datetime\nimport time\nfrom glob import glob\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nfrom PIL import Image\nimport torch\nimport numpy as np\nfrom ultralytics import RTDETR\nfrom ultralytics.data.build import YOLODataset\nimport ultralytics.data.build as build\ndevice='cuda'\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T09:12:23.444213Z","iopub.execute_input":"2025-05-13T09:12:23.444897Z","iopub.status.idle":"2025-05-13T09:12:28.127641Z","shell.execute_reply.started":"2025-05-13T09:12:23.444868Z","shell.execute_reply":"2025-05-13T09:12:28.127122Z"}},"outputs":[{"name":"stdout","text":"Creating new Ultralytics Settings v0.0.6 file ‚úÖ \nView Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\nUpdate Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"### üìÅ  Set configurations and seed","metadata":{}},{"cell_type":"code","source":"class CFG:\n    seed = 42\n    random_state = 42\n    folds=10\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    torch.use_deterministic_algorithms(True, warn_only=True)\nseed_everything(CFG.seed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T09:12:28.128316Z","iopub.execute_input":"2025-05-13T09:12:28.128667Z","iopub.status.idle":"2025-05-13T09:12:28.139349Z","shell.execute_reply.started":"2025-05-13T09:12:28.128648Z","shell.execute_reply":"2025-05-13T09:12:28.138663Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Read the CSV File\ndf = pd.read_csv('/kaggle/input/amini-cocoa-contamination-challenge/Train.csv')\n\n# Extract Unique Class Labels\nunique_classes = df['class'].unique()\n\n# Create a Class-to-Index Mapping\nclass_mapping = {cls: idx for idx, cls in enumerate(unique_classes)}\nprint(class_mapping)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T09:12:28.140763Z","iopub.execute_input":"2025-05-13T09:12:28.140995Z","iopub.status.idle":"2025-05-13T09:12:31.192213Z","shell.execute_reply.started":"2025-05-13T09:12:28.140973Z","shell.execute_reply":"2025-05-13T09:12:31.191585Z"}},"outputs":[{"name":"stdout","text":"{'healthy': 0, 'anthracnose': 1, 'cssvd': 2}\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"### ‚úÖ Purpose\nThis is a common preprocessing step in machine learning tasks to convert categorical class labels into numerical values that can be used for model training.","metadata":{}},{"cell_type":"markdown","source":"### üìÅ Directory and Data Loading","metadata":{}},{"cell_type":"code","source":"# Set the data directory\nDATA_DIR = Path('/kaggle/input/amini-cocoa-contamination-challenge/')\nIMGS_DIR = Path('/kaggle/input/amini-cocoa-contamination-challenge/dataset/images')\n\n# Load train and test files\ntrain = pd.read_csv(DATA_DIR / 'Train.csv')\ntest = pd.read_csv(DATA_DIR / 'Test.csv')\nss = pd.read_csv(DATA_DIR / 'SampleSubmission.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T09:12:31.192783Z","iopub.execute_input":"2025-05-13T09:12:31.193011Z","iopub.status.idle":"2025-05-13T09:12:31.239320Z","shell.execute_reply.started":"2025-05-13T09:12:31.192994Z","shell.execute_reply":"2025-05-13T09:12:31.238795Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"### ‚úÖ Purpose\nThis setup organizes paths and loads necessary CSV files for training, inference, and submission formatting in a Kaggle environment.","metadata":{}},{"cell_type":"markdown","source":"### üìÅ Let's explore the Train, Test, Sample Submission File","metadata":{}},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T09:12:31.240034Z","iopub.execute_input":"2025-05-13T09:12:31.240316Z","iopub.status.idle":"2025-05-13T09:12:31.262799Z","shell.execute_reply.started":"2025-05-13T09:12:31.240294Z","shell.execute_reply":"2025-05-13T09:12:31.262270Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"        Image_ID    class  confidence   ymin   xmin   ymax   xmax  class_id  \\\n0  ID_nBgcAR.jpg  healthy         1.0   75.0   15.0  162.0  195.0         2   \n1  ID_nBgcAR.jpg  healthy         1.0   58.0    1.0  133.0  171.0         2   \n2  ID_nBgcAR.jpg  healthy         1.0   42.0   29.0  377.0  349.0         2   \n3  ID_Kw2v8A.jpg  healthy         1.0  112.0  124.0  404.0  341.0         2   \n4  ID_Kw2v8A.jpg  healthy         1.0  148.0  259.0  413.0  412.0         2   \n\n                            ImagePath  \n0  dataset/images/train/ID_nBgcAR.jpg  \n1  dataset/images/train/ID_nBgcAR.jpg  \n2  dataset/images/train/ID_nBgcAR.jpg  \n3  dataset/images/train/ID_Kw2v8A.jpg  \n4  dataset/images/train/ID_Kw2v8A.jpg  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Image_ID</th>\n      <th>class</th>\n      <th>confidence</th>\n      <th>ymin</th>\n      <th>xmin</th>\n      <th>ymax</th>\n      <th>xmax</th>\n      <th>class_id</th>\n      <th>ImagePath</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ID_nBgcAR.jpg</td>\n      <td>healthy</td>\n      <td>1.0</td>\n      <td>75.0</td>\n      <td>15.0</td>\n      <td>162.0</td>\n      <td>195.0</td>\n      <td>2</td>\n      <td>dataset/images/train/ID_nBgcAR.jpg</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ID_nBgcAR.jpg</td>\n      <td>healthy</td>\n      <td>1.0</td>\n      <td>58.0</td>\n      <td>1.0</td>\n      <td>133.0</td>\n      <td>171.0</td>\n      <td>2</td>\n      <td>dataset/images/train/ID_nBgcAR.jpg</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ID_nBgcAR.jpg</td>\n      <td>healthy</td>\n      <td>1.0</td>\n      <td>42.0</td>\n      <td>29.0</td>\n      <td>377.0</td>\n      <td>349.0</td>\n      <td>2</td>\n      <td>dataset/images/train/ID_nBgcAR.jpg</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ID_Kw2v8A.jpg</td>\n      <td>healthy</td>\n      <td>1.0</td>\n      <td>112.0</td>\n      <td>124.0</td>\n      <td>404.0</td>\n      <td>341.0</td>\n      <td>2</td>\n      <td>dataset/images/train/ID_Kw2v8A.jpg</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ID_Kw2v8A.jpg</td>\n      <td>healthy</td>\n      <td>1.0</td>\n      <td>148.0</td>\n      <td>259.0</td>\n      <td>413.0</td>\n      <td>412.0</td>\n      <td>2</td>\n      <td>dataset/images/train/ID_Kw2v8A.jpg</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"test.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T09:12:31.263455Z","iopub.execute_input":"2025-05-13T09:12:31.263663Z","iopub.status.idle":"2025-05-13T09:12:31.273755Z","shell.execute_reply.started":"2025-05-13T09:12:31.263647Z","shell.execute_reply":"2025-05-13T09:12:31.272939Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"        Image_ID  class  confidence  ymin  xmin  ymax  xmax  class_id  \\\n0  ID_Genxyu.jpg    NaN         NaN   NaN   NaN   NaN   NaN       NaN   \n1  ID_svY6TG.jpg    NaN         NaN   NaN   NaN   NaN   NaN       NaN   \n2  ID_d0gpda.jpg    NaN         NaN   NaN   NaN   NaN   NaN       NaN   \n3  ID_frWmBT.jpg    NaN         NaN   NaN   NaN   NaN   NaN       NaN   \n4  ID_TaRW6o.jpg    NaN         NaN   NaN   NaN   NaN   NaN       NaN   \n\n                           ImagePath  \n0  dataset/images/test/ID_Genxyu.jpg  \n1  dataset/images/test/ID_svY6TG.jpg  \n2  dataset/images/test/ID_d0gpda.jpg  \n3  dataset/images/test/ID_frWmBT.jpg  \n4  dataset/images/test/ID_TaRW6o.jpg  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Image_ID</th>\n      <th>class</th>\n      <th>confidence</th>\n      <th>ymin</th>\n      <th>xmin</th>\n      <th>ymax</th>\n      <th>xmax</th>\n      <th>class_id</th>\n      <th>ImagePath</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ID_Genxyu.jpg</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>dataset/images/test/ID_Genxyu.jpg</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ID_svY6TG.jpg</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>dataset/images/test/ID_svY6TG.jpg</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ID_d0gpda.jpg</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>dataset/images/test/ID_d0gpda.jpg</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ID_frWmBT.jpg</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>dataset/images/test/ID_frWmBT.jpg</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ID_TaRW6o.jpg</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>dataset/images/test/ID_TaRW6o.jpg</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"ss.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T09:12:31.274581Z","iopub.execute_input":"2025-05-13T09:12:31.274839Z","iopub.status.idle":"2025-05-13T09:12:31.292545Z","shell.execute_reply.started":"2025-05-13T09:12:31.274810Z","shell.execute_reply":"2025-05-13T09:12:31.291828Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"        Image_ID    class  confidence  ymin  xmin  ymax  xmax\n0  ID_Genxyu.jpg  healthy         0.5   100   100   100   100\n1  ID_svY6TG.jpg  healthy         0.5   100   100   100   100\n2  ID_d0gpda.jpg  healthy         0.5   100   100   100   100\n3  ID_frWmBT.jpg  healthy         0.5   100   100   100   100\n4  ID_TaRW6o.jpg  healthy         0.5   100   100   100   100","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Image_ID</th>\n      <th>class</th>\n      <th>confidence</th>\n      <th>ymin</th>\n      <th>xmin</th>\n      <th>ymax</th>\n      <th>xmax</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ID_Genxyu.jpg</td>\n      <td>healthy</td>\n      <td>0.5</td>\n      <td>100</td>\n      <td>100</td>\n      <td>100</td>\n      <td>100</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ID_svY6TG.jpg</td>\n      <td>healthy</td>\n      <td>0.5</td>\n      <td>100</td>\n      <td>100</td>\n      <td>100</td>\n      <td>100</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ID_d0gpda.jpg</td>\n      <td>healthy</td>\n      <td>0.5</td>\n      <td>100</td>\n      <td>100</td>\n      <td>100</td>\n      <td>100</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ID_frWmBT.jpg</td>\n      <td>healthy</td>\n      <td>0.5</td>\n      <td>100</td>\n      <td>100</td>\n      <td>100</td>\n      <td>100</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ID_TaRW6o.jpg</td>\n      <td>healthy</td>\n      <td>0.5</td>\n      <td>100</td>\n      <td>100</td>\n      <td>100</td>\n      <td>100</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"Clearly, the test set does not have values for the bounding boxes and confidence which makes sense because we are to find them.\n\nLet's explore furthermore","metadata":{}},{"cell_type":"markdown","source":"### üè∑Ô∏è Checking for Duplicates","metadata":{}},{"cell_type":"code","source":"# Defininig columns in the train set\ndefined_cols = ['Image_ID',\t'confidence',\t'class',\t'ymin',\t'xmin',\t'ymax',\t'xmax']\ntrain = train[defined_cols]\n\nprint(f'Sum of duplicated colums: {train.duplicated().sum()}')\nprint(f'Size of dataframe before removing duplicates: {train.shape}')\n\n# Remove duplicates\ntrain = train.drop_duplicates()\nprint(f'Sum of duplicated colums after removing duplicates: {train.duplicated().sum()}')\nprint(f'Size of dataframe after removing duplicates: {train.shape}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T09:12:31.293294Z","iopub.execute_input":"2025-05-13T09:12:31.293578Z","iopub.status.idle":"2025-05-13T09:12:31.327950Z","shell.execute_reply.started":"2025-05-13T09:12:31.293553Z","shell.execute_reply":"2025-05-13T09:12:31.327427Z"}},"outputs":[{"name":"stdout","text":"Sum of duplicated colums: 0\nSize of dataframe before removing duplicates: (9792, 7)\nSum of duplicated colums after removing duplicates: 0\nSize of dataframe after removing duplicates: (9792, 7)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"### üè∑Ô∏è Generating Class Label Dictionary","metadata":{}},{"cell_type":"code","source":"unique_classes = train['class'].unique()\nfull_label_dict = {cls: idx for idx, cls in enumerate(unique_classes)}\nfull_label_dict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T09:12:31.330359Z","iopub.execute_input":"2025-05-13T09:12:31.330550Z","iopub.status.idle":"2025-05-13T09:12:31.335901Z","shell.execute_reply.started":"2025-05-13T09:12:31.330536Z","shell.execute_reply":"2025-05-13T09:12:31.335294Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"{'healthy': 0, 'anthracnose': 1, 'cssvd': 2}"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"### ‚úÖ Purpose\nConverts categorical class labels into numerical indices for use in model training and evaluation.","metadata":{}},{"cell_type":"markdown","source":"## üß™ Data Preparation and Stratified K-Fold Setup for Multi-Label Classification","metadata":{}},{"cell_type":"markdown","source":"### Group Labels by Image","metadata":{}},{"cell_type":"code","source":"# Step 1: Group by Image_ID and aggregate class labels into lists\ntrain['new_class'] = train['class'].map(full_label_dict)\ngrouped = train.groupby('Image_ID')['new_class'].apply(list).reset_index()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T09:12:31.336505Z","iopub.execute_input":"2025-05-13T09:12:31.336709Z","iopub.status.idle":"2025-05-13T09:12:31.430646Z","shell.execute_reply.started":"2025-05-13T09:12:31.336695Z","shell.execute_reply":"2025-05-13T09:12:31.430127Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"### ‚úÖ Purpose\n1. Maps class labels to integers using full_label_dict.\n2. Groups the data by Image_ID, aggregating all associated class labels into lists.\n3. This ensures each image has a list of all labels assigned to it (for multi-label classification).\n","metadata":{}},{"cell_type":"markdown","source":"### Initialize Class Columns with -1","metadata":{}},{"cell_type":"code","source":"all_classes = train[\"class\"].unique().tolist()\nfor unique_class in all_classes:\n    grouped[unique_class] = -1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T09:12:31.431429Z","iopub.execute_input":"2025-05-13T09:12:31.431882Z","iopub.status.idle":"2025-05-13T09:12:31.437125Z","shell.execute_reply.started":"2025-05-13T09:12:31.431860Z","shell.execute_reply":"2025-05-13T09:12:31.436300Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"### ‚úÖ Purpose\n1. Retrieves the list of all unique class names from the training data.\n\n2. Adds a column for each class in the grouped DataFrame.\n\n3. Initializes these columns to -1, which will later be updated to 1 or 0 based on label presence.","metadata":{}},{"cell_type":"markdown","source":"### Reverse Label Mapping","metadata":{}},{"cell_type":"code","source":"reverse_label_mapping = {full_label_dict[key]:key for key in full_label_dict} ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T09:12:31.437934Z","iopub.execute_input":"2025-05-13T09:12:31.438167Z","iopub.status.idle":"2025-05-13T09:12:31.452433Z","shell.execute_reply.started":"2025-05-13T09:12:31.438151Z","shell.execute_reply":"2025-05-13T09:12:31.451878Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"reverse_label_mapping","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T09:12:31.453029Z","iopub.execute_input":"2025-05-13T09:12:31.453251Z","iopub.status.idle":"2025-05-13T09:12:31.465879Z","shell.execute_reply.started":"2025-05-13T09:12:31.453235Z","shell.execute_reply":"2025-05-13T09:12:31.465241Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"{0: 'healthy', 1: 'anthracnose', 2: 'cssvd'}"},"metadata":{}}],"execution_count":14},{"cell_type":"markdown","source":"### ‚úÖ Purpose\n1. Creates a dictionary to map numeric labels back to their original string labels.\n\n2. This is useful for labeling class columns correctly in the next step.","metadata":{}},{"cell_type":"markdown","source":"### One-Hot Encode Labels for Each Image","metadata":{}},{"cell_type":"code","source":"# input 1 if the label is in that image else 0\nall_labels_list = (list(grouped['new_class'].values))\nfor train_index, label_List in enumerate(all_labels_list):\n    unique_labels = list(set(label_List))\n    for label_index in range(len(unique_labels)):\n        label = int(unique_labels[label_index])\n        for key_value in range(23):\n            if label == key_value:\n                grouped.loc[train_index, reverse_label_mapping[key_value]] = 1\n                break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T09:12:31.466549Z","iopub.execute_input":"2025-05-13T09:12:31.466790Z","iopub.status.idle":"2025-05-13T09:12:32.078681Z","shell.execute_reply.started":"2025-05-13T09:12:31.466767Z","shell.execute_reply":"2025-05-13T09:12:32.078126Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"### ‚úÖ Purpose\n1. Iterates through each image's list of labels.\n\n2. Converts the list into a set of unique labels.\n\n3. For each label, checks if it matches a known label index.\n\nIf matched, sets the corresponding column in grouped to 1, indicating presence of that class label for that image.","metadata":{}},{"cell_type":"markdown","source":"### Create Stratified Folds and Image Paths","metadata":{}},{"cell_type":"code","source":"X = grouped[['Image_ID']]\ngrouped['fold'] = -1\nmskf = MultilabelStratifiedKFold(n_splits=CFG.folds, shuffle=True, random_state=CFG.random_state)\nfor i_fold, (train_index, test_index) in enumerate(mskf.split(X, grouped[all_classes])):\n    grouped.loc[test_index, \"fold\"] = i_fold     ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T09:12:32.079320Z","iopub.execute_input":"2025-05-13T09:12:32.079504Z","iopub.status.idle":"2025-05-13T09:12:32.272893Z","shell.execute_reply.started":"2025-05-13T09:12:32.079490Z","shell.execute_reply":"2025-05-13T09:12:32.272370Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"### ‚úÖ Purpose\n1. Initializes a MultilabelStratifiedKFold for balanced fold generation.\n\n2. Splits the data so that each fold has a similar distribution of labels across classes.\n\n3. Assigns a fold number to each image for use in cross-validation.","metadata":{}},{"cell_type":"code","source":"# create image_path for grouped_data\ngrouped['image_path'] = [Path(str(IMGS_DIR) + '/train/' + x) for x in grouped.Image_ID]\n\n# drop duplicates rows for test\ntest = test.drop_duplicates(subset=['Image_ID'], ignore_index=True)\ntest['image_path'] = [Path(str(IMGS_DIR) + '/test/' + x) for x in test.Image_ID]  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T09:12:32.273549Z","iopub.execute_input":"2025-05-13T09:12:32.273773Z","iopub.status.idle":"2025-05-13T09:12:32.321966Z","shell.execute_reply.started":"2025-05-13T09:12:32.273756Z","shell.execute_reply":"2025-05-13T09:12:32.321231Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"## üìù Function to Convert Bounding Boxes to YOLO Format and Save Annotations\n\n---\n\n### üì¶ Function: `save_yolo_annotation`","metadata":{}},{"cell_type":"markdown","source":"All values are normalized by the image width and height.\n\n### Steps:\n\n1. Reads the image to get its dimensions.\n\n2. Normalizes bounding box coordinates.\n\n3. Writes a .txt label file for the corresponding image in YOLO format.\n\n4. Failsafe: If the image cannot be read, an error is raised.","metadata":{}},{"cell_type":"code","source":"# Function to convert the bounding boxes to YOLO format and save them\ndef save_yolo_annotation(row):\n\n    image_path, class_id, output_dir = row['image_path'], row['class_id'], row['output_dir']\n\n    img = cv2.imread(str(image_path))\n    if img is None:\n        raise ValueError(f\"Could not read image from path: {image_path}\")\n\n    height, width, _ = img.shape\n    label_file = Path(output_dir) / f\"{Path(image_path).stem}.txt\"\n\n\n    ymin, xmin, ymax, xmax = row['ymin'], row['xmin'], row['ymax'], row['xmax']\n\n    # Normalize the coordinates\n    x_center = (xmin + xmax) / 2 / width\n    y_center = (ymin + ymax) / 2 / height\n    bbox_width = (xmax - xmin) / width\n    bbox_height = (ymax - ymin) / height\n\n    with open(label_file, 'a') as f:\n        f.write(f\"{class_id} {x_center:.6f} {y_center:.6f} {bbox_width:.6f} {bbox_height:.6f}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T09:12:32.323010Z","iopub.execute_input":"2025-05-13T09:12:32.323299Z","iopub.status.idle":"2025-05-13T09:12:32.328348Z","shell.execute_reply.started":"2025-05-13T09:12:32.323276Z","shell.execute_reply":"2025-05-13T09:12:32.327645Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"### üîç Explanation\nPurpose: Converts bounding boxes from (xmin, ymin, xmax, ymax) format to the YOLO format:\n```arduino\nclass_id x_center y_center width height\n```","metadata":{}},{"cell_type":"code","source":"# Parallelize the annotation saving process\ndef process_dataset(dataframe, output_dir):\n    dataframe['output_dir'] = output_dir\n    # convert the dataframe to a dictionary\n    dataframe = dataframe.to_dict('records')\n    for i in tqdm(range(len(dataframe))):\n        save_yolo_annotation(dataframe[i])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T09:12:32.328922Z","iopub.execute_input":"2025-05-13T09:12:32.329171Z","iopub.status.idle":"2025-05-13T09:12:32.345619Z","shell.execute_reply.started":"2025-05-13T09:12:32.329151Z","shell.execute_reply":"2025-05-13T09:12:32.344888Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"### üîç Explanation\n1. Adds an output_dir column to the DataFrame for storing annotation files.\n\n2. Converts the DataFrame to a list of dictionaries (records) for easier iteration.\n\n3. Iterates over each row and calls save_yolo_annotation to save YOLO labels.\n\n4. A progress bar (tqdm) tracks the process for better visibility.","metadata":{}},{"cell_type":"markdown","source":"#### Let's Implement this now!","metadata":{}},{"cell_type":"code","source":"# # Add an image_path column\ntrain['image_path'] = [Path(str(IMGS_DIR) + '/train/' + x) for x in train.Image_ID]\n\n# Map string classes to integers (label encoding targets)\ntrain['class_id'] = train['class'].map(full_label_dict)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T09:12:32.346420Z","iopub.execute_input":"2025-05-13T09:12:32.346948Z","iopub.status.idle":"2025-05-13T09:12:32.412404Z","shell.execute_reply.started":"2025-05-13T09:12:32.346926Z","shell.execute_reply":"2025-05-13T09:12:32.411567Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"## üîÅ Preparing YOLO Training/Validation/Test Data for a Specific Fold\n\nThis code processes one fold (specifically `fold == 1`) of your dataset to organize images and labels into the required format for training a YOLO model.\n\n---","metadata":{}},{"cell_type":"code","source":"# üß† Loop Through Folds\n\nfor fold in range(CFG.folds):\n    if fold == 1:\n        # üìÅ Define Directory Structure\n        # images\n        TRAIN_IMAGES_DIR = Path(f'/kaggle/working/train/images/fold_{fold + 1}')\n        VAL_IMAGES_DIR = Path(f'/kaggle/working/val/images/fold_{fold + 1}')\n        TEST_IMAGES_DIR = Path('/kaggle/working/test/images')\n\n        # labels\n        TRAIN_LABELS_DIR = Path(f'/kaggle/working/train/labels/fold_{fold + 1}')\n        VAL_LABELS_DIR = Path(f'/kaggle/working/val/labels/fold_{fold + 1}')\n        TEST_LABELS_DIR = Path('/kaggle/working/test/labels')\n\n        # Get the train and val for that fold\n        train_fold = grouped[grouped['fold'] != fold ].reset_index(drop=True)\n        val_fold = grouped[grouped['fold'] == fold].reset_index(drop=True)\n\n        DIRS = [TRAIN_IMAGES_DIR, VAL_IMAGES_DIR, TRAIN_LABELS_DIR, VAL_LABELS_DIR, TEST_IMAGES_DIR, TEST_LABELS_DIR]\n        \n        # Create necessary directories\n        for DIR in DIRS:\n            if DIR.exists():\n                shutil.rmtree(DIR)\n            DIR.mkdir(parents=True, exist_ok=True)\n       \n        # Copy train, val, and test images to their respective dirs\n        for img in tqdm(train_fold.image_path.unique()):\n            shutil.copy(img, TRAIN_IMAGES_DIR / img.parts[-1])\n        print(f'Copied train file for fold{fold+1} to folder')\n\n        for img in tqdm(val_fold.image_path.unique()):\n            shutil.copy(img, VAL_IMAGES_DIR / img.parts[-1])\n        print(f'Copied val file for fold{fold+1} to folder')\n\n        for img in tqdm(test.image_path.unique()):\n            shutil.copy(img, TEST_IMAGES_DIR / img.parts[-1])\n        print(f'Copied test file for first fold to folder')\n\n\n        X_train = train[train.Image_ID.isin(train_fold.Image_ID)].reset_index(drop=True)\n        X_val = train[train.Image_ID.isin(val_fold.Image_ID)].reset_index(drop=True)\n\n\n        print(f\"-------------Process Datasets for fold {fold+1}\")\n        # Save train and validation labels to their respective dirs\n        process_dataset(X_train, TRAIN_LABELS_DIR)\n        process_dataset(X_val, VAL_LABELS_DIR)\n\n        print(f\"-------------End of Processing of Datasets for fold {fold+1}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T09:12:32.413288Z","iopub.execute_input":"2025-05-13T09:12:32.413526Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4976 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2d7c07d6df34255b9b2399ebf995746"}},"metadata":{}},{"name":"stdout","text":"Copied train file for fold2 to folder\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/553 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fae655cfefc947d2bd49597c0ed74061"}},"metadata":{}},{"name":"stdout","text":"Copied val file for fold2 to folder\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1626 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13d08f7437404c2b9560cbb4792f7259"}},"metadata":{}},{"name":"stdout","text":"Copied test file for first fold to folder\n-------------Process Datasets for fold 2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/8849 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf46432ce0fd436399bc0aa36e6a36b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/943 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf1191dc847e49b7a10534c9919942a9"}},"metadata":{}}],"execution_count":null},{"cell_type":"markdown","source":"### ‚úÖ Summary\nThis fold-wise script performs the following:\n\nSets up directories for training, validation, and test images and labels.\n\nSplits the dataset by fold.\n\nCopies image files into YOLO-style directories.\n\nConverts and saves labels in YOLO format using the process_dataset function.\n\nPrepares data for training object detection models like YOLOv5 or YOLOv8.\n\n‚ÑπÔ∏è Only fold == 1 is processed in this script, which can be extended to process all folds by removing the if fold == 1 condition.","metadata":{}},{"cell_type":"markdown","source":"### This is Optional. If you are using Kaggle, run this cell to avoid getting errors","metadata":{}},{"cell_type":"code","source":"# Define the new dataset directory structure within the current working directory\nbase_dir = './datasets'  # Create the dataset in the local writable directory\ndirs = [\n    os.path.join(base_dir, 'train/images'),\n    os.path.join(base_dir, 'train/labels'),\n    os.path.join(base_dir, 'val/images'),\n    os.path.join(base_dir, 'val/labels')\n]\n\n# Create the directories\nfor dir_path in dirs:\n    os.makedirs(dir_path, exist_ok=True)\n\n# Example: Source directories where your current files are stored (update these paths)\nsource_train_images = './train/images'\nsource_train_labels = './train/labels'\nsource_val_images = './val/images'\nsource_val_labels = './val/labels'\n\n# Move files to the new structure\ndef move_files(source, destination):\n    if os.path.exists(source):\n        for file_name in os.listdir(source):\n            shutil.move(os.path.join(source, file_name), destination)\n\n# Move training images and labels\nmove_files(source_train_images, os.path.join(base_dir, 'train/images'))\nmove_files(source_train_labels, os.path.join(base_dir, 'train/labels'))\n\n# Move validation images and labels\nmove_files(source_val_images, os.path.join(base_dir, 'val/images'))\nmove_files(source_val_labels, os.path.join(base_dir, 'val/labels'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T09:27:18.834676Z","iopub.execute_input":"2025-05-13T09:27:18.834937Z","iopub.status.idle":"2025-05-13T09:27:18.842808Z","shell.execute_reply.started":"2025-05-13T09:27:18.834921Z","shell.execute_reply":"2025-05-13T09:27:18.841926Z"}},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":"## üìÑ Create `data.yaml` File for YOLO Training\n\nThis script creates the `data.yaml` file, which is required by YOLO models (such as YOLOv11 or YOLOv12) to define the training configuration.","metadata":{}},{"cell_type":"code","source":"# Create a data.yaml file required by YOLO\n# üè∑Ô∏è Extract Class Names and Count\nclass_names = train['class'].unique().tolist()\nnum_classes = len(class_names)\n\n# üìÅ Define Training and Validation Image Directories\n# images\nTRAIN_IMAGES_DIR = Path('/kaggle/working/datasets/train/images/fold_2/')\nVAL_IMAGES_DIR = Path('/kaggle/working/datasets/val/images/fold_2/')\n\n#üìò Build YAML Configuration Dictionary\ndata_yaml = {\n    'train': str(TRAIN_IMAGES_DIR),\n    'val': str(VAL_IMAGES_DIR),\n    'nc': num_classes,\n    'names': class_names\n}\n\n# Save the data.yaml file\nyaml_path = Path('data.yaml')\nwith open(yaml_path, 'w') as file:\n    yaml.dump(data_yaml, file, default_flow_style=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T09:27:22.996342Z","iopub.execute_input":"2025-05-13T09:27:22.996622Z","iopub.status.idle":"2025-05-13T09:27:23.003792Z","shell.execute_reply.started":"2025-05-13T09:27:22.996602Z","shell.execute_reply":"2025-05-13T09:27:23.003132Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"### ‚úÖ Purpose\nConstructs the dictionary for the data.yaml file, containing:\n\ntrain: path to training images.\n\nval: path to validation images.\n\nnc: number of classes.\n\nnames: list of class names (in order).\n\nSaves the constructed dictionary into a data.yaml file.\n\nThis file will be used by YOLO during training to understand dataset structure and class names.","metadata":{}},{"cell_type":"code","source":"data_yaml","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T09:27:27.712654Z","iopub.execute_input":"2025-05-13T09:27:27.712926Z","iopub.status.idle":"2025-05-13T09:27:27.718884Z","shell.execute_reply.started":"2025-05-13T09:27:27.712906Z","shell.execute_reply":"2025-05-13T09:27:27.718064Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"{'train': '/kaggle/working/datasets/train/images/fold_2',\n 'val': '/kaggle/working/datasets/val/images/fold_2',\n 'nc': 3,\n 'names': ['healthy', 'anthracnose', 'cssvd']}"},"metadata":{}}],"execution_count":27},{"cell_type":"markdown","source":"## üß† Train First Model, A YOLOv11 Model on Custom Dataset\n\nThis code snippet initializes a YOLOv11 model and trains it using the configuration specified in the `data.yaml` file.\n","metadata":{}},{"cell_type":"markdown","source":"### üöÄ Hyperparameters\n```yaml\ndata: Path to the data.yaml file describing the dataset.\n\nepochs: Train for 100 full passes over the training dataset.\n\nimgsz: Resize input images to 576√ó576 pixels.\n\ndevice: Use GPU device 0 for training.\n\nbatch: Batch size of 16 images per step.\n\noptimizer: Use the AdamW optimizer (better regularization).\n\nlr0: Initial learning rate of 3e-4.\n\nmomentum: Momentum factor for optimizer (used if switching to SGD).\n\nweight_decay: L2 regularization to prevent overfitting.\n\nclose_mosaic: Disable YOLO‚Äôs mosaic augmentation after 30 epochs (stabilizes training).\n\nseed: Set a fixed random seed (42) for reproducibility.\n\npatience: Set to stop the training after no improvement\n```","metadata":{}},{"cell_type":"code","source":"# üîß Load Pretrained YOLOv11 Model\nmodel1 = YOLO(\"yolo11l.pt\")\n\n# üöÄ Simulate 100-Epoch Training in 46 Epochs\nmodel1.train(data='data.yaml',\n             epochs=100,               \n             imgsz=640,\n             device=0,\n             batch=16,\n             optimizer='AdamW',\n             lr0=3e-4,           \n             momentum=0.9,\n             weight_decay=1e-2,\n             close_mosaic=30,\n             seed=42,\n             patience=10      \n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### üìà Outcome\nThis setup fine-tunes the YOLOv11 large model on your custom dataset, leveraging strong regularization and augmentation settings to improve generalization. After training, the best model checkpoint will be saved and can be used for inference or further evaluation.","metadata":{}},{"cell_type":"markdown","source":"## ‚úÖ Evaluate the Trained YOLOv11 Model\n\nAfter training, we evaluate the model's performance on the validation dataset using the `.val()` method.","metadata":{}},{"cell_type":"code","source":"results = model1.val()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T09:27:37.448761Z","iopub.execute_input":"2025-05-13T09:27:37.449345Z","iopub.status.idle":"2025-05-13T09:28:10.291138Z","shell.execute_reply.started":"2025-05-13T09:27:37.449315Z","shell.execute_reply":"2025-05-13T09:28:10.290444Z"}},"outputs":[{"name":"stdout","text":"Ultralytics 8.3.133 üöÄ Python-3.11.11 torch-2.5.1+cu124 CUDA:0 (Tesla T4, 15095MiB)\nYOLO11l summary (fused): 190 layers, 25,281,625 parameters, 0 gradients, 86.6 GFLOPs\nDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 755k/755k [00:00<00:00, 18.0MB/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 2471.5¬±1298.1 MB/s, size: 1661.7 KB)\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/working/datasets/val/labels/fold_2... 553 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 553/553 [00:03<00:00, 169.13it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_BFveJq.jpeg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_By57N4.jpeg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_EJWqGf.jpeg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_EUJ6CX.jpeg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_FtYRqz.jpeg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_HuMwmi.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_J7hL2Y.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_NGiOVN.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_SuY5t1.jpeg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_YevJzo.jpeg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_Zwe8i7.jpeg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_bCoAZg.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_bouUUL.jpeg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_f22xL0.jpeg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_jbDUGG.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_jkpHUf.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_k7E9yz.jpeg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_krz5Kc.jpeg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_mhIu8M.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_oOXqz9.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_q0MS9f.jpeg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_sZNOcT.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_soB3nB.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_teXsR2.jpeg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_uKHtOl.jpeg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_uiDeaI.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_unaCV7.jpeg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_y31JbP.jpeg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_yol9H8.jpeg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_z6gC1L.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /kaggle/working/datasets/val/labels/fold_2.cache\n","output_type":"stream"},{"name":"stderr","text":"\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 35/35 [00:18<00:00,  1.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        553        943      0.788      0.751      0.821      0.571\n               healthy        172        386      0.744      0.731      0.801      0.549\n           anthracnose        136        194      0.808      0.803      0.862      0.612\n                 cssvd        246        363      0.811      0.719        0.8      0.553\nSpeed: 0.2ms preprocess, 22.9ms inference, 0.0ms loss, 1.8ms postprocess per image\nResults saved to \u001b[1mruns/detect/val\u001b[0m\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"Our Cross Validation is at 0.821 which is very great. Let's train another model and ensemble it.","metadata":{}},{"cell_type":"markdown","source":"Save best.pt for inference","metadata":{}},{"cell_type":"markdown","source":"## üß† Train Second Model, A YOLOv11 Model on Custom YoloWeightedDataset\nThis code snippet initializes a YOLOv11 model and trains it using the configuration specified in the data.yaml file. A Yolo weighted dataset is utilized.","metadata":{}},{"cell_type":"markdown","source":"The YOLOWeightedDataset class tries to balance the custom for dataset training the model.","metadata":{}},{"cell_type":"markdown","source":"I followed a similar training pipeline to the one used in the [Ghana Crop Disease Detection Challenge Notebook by Raphael Kiminya](https://github.com/kiminya-raphael/Ghana-Crop-Disease-Detection-challenge/blob/main/1_train.ipynb), making modifications specific to this dataset and task.\n","metadata":{}},{"cell_type":"code","source":"class YOLOWeightedDataset(YOLODataset):\n    def __init__(self, *args, mode=\"train\", **kwargs):\n        \"\"\"\n        Initialize the WeightedDataset.\n\n        Args:\n            class_weights (list or numpy array): A list or array of weights corresponding to each class.\n        \"\"\"\n\n        super(YOLOWeightedDataset, self).__init__(*args, **kwargs)\n\n        self.train_mode = \"train\" in self.prefix\n\n        # You can also specify weights manually instead\n        self.count_instances()\n        class_weights = np.sum(self.counts) / self.counts\n\n        # Aggregation function\n        self.agg_func = np.mean\n\n        self.class_weights = np.array(class_weights)\n        self.weights = self.calculate_weights()\n        self.probabilities = self.calculate_probabilities()\n\n    def count_instances(self):\n        \"\"\"\n        Count the number of instances per class\n\n        Returns:\n            dict: A dict containing the counts for each class.\n        \"\"\"\n        self.counts = [0 for i in range(len(self.data[\"names\"]))]\n        for label in self.labels:\n            cls = label['cls'].reshape(-1).astype(int)\n            for id in cls:\n                self.counts[id] += 1\n\n        self.counts = np.array(self.counts)\n        self.counts = np.where(self.counts == 0, 1, self.counts)\n\n    def calculate_weights(self):\n        \"\"\"\n        Calculate the aggregated weight for each label based on class weights.\n\n        Returns:\n            list: A list of aggregated weights corresponding to each label.\n        \"\"\"\n        weights = []\n        for label in self.labels:\n            cls = label['cls'].reshape(-1).astype(int)\n\n            # Give a default weight to background class\n            if cls.size == 0:\n              weights.append(1)\n              continue\n\n            # Take mean of weights\n            # You can change this weight aggregation function to aggregate weights differently\n            weight = self.agg_func(self.class_weights[cls])\n            weights.append(weight)\n        return weights\n\n    def calculate_probabilities(self):\n        \"\"\"\n        Calculate and store the sampling probabilities based on the weights.\n\n        Returns:\n            list: A list of sampling probabilities corresponding to each label.\n        \"\"\"\n        total_weight = sum(self.weights)\n        probabilities = [w / total_weight for w in self.weights]\n        return probabilities\n\n    def __getitem__(self, index):\n        \"\"\"\n        Return transformed label information based on the sampled index.\n        \"\"\"\n        # Don't use for validation\n        if not self.train_mode:\n            return self.transforms(self.get_image_and_label(index))\n        else:\n            index = np.random.choice(len(self.labels), p=self.probabilities)\n            return self.transforms(self.get_image_and_label(index))\n\nbuild.YOLODataset = YOLOWeightedDataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Just like the first model, we used the same model with its respective hyperparameters","metadata":{}},{"cell_type":"code","source":"# üîß Load Pretrained YOLOv11 Model\nmodel2 = YOLO(\"yolo11l.pt\")\n\n# üöÄ Simulate 100-Epoch Training in 46 Epochs\nmodel2.train(data='data.yaml',\n             epochs=100,               \n             imgsz=640,\n             device=0,\n             batch=16,\n             optimizer='AdamW',\n             lr0=3e-4,           \n             momentum=0.9,\n             weight_decay=1e-2,\n             close_mosaic=30,\n             seed=42,\n             patience=10      \n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"After training, we evaluate the model's performance on the validation dataset using the `.val()` method.","metadata":{}},{"cell_type":"code","source":"results = model2.val()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T09:28:17.937721Z","iopub.execute_input":"2025-05-13T09:28:17.938226Z","iopub.status.idle":"2025-05-13T09:28:40.210628Z","shell.execute_reply.started":"2025-05-13T09:28:17.938197Z","shell.execute_reply":"2025-05-13T09:28:40.209758Z"}},"outputs":[{"name":"stdout","text":"Ultralytics 8.3.133 üöÄ Python-3.11.11 torch-2.5.1+cu124 CUDA:0 (Tesla T4, 15095MiB)\nYOLO11l summary (fused): 190 layers, 25,281,625 parameters, 0 gradients, 86.6 GFLOPs\n\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 2015.1¬±557.3 MB/s, size: 728.6 KB)\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/working/datasets/val/labels/fold_2.cache... 553 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 553/553 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_BFveJq.jpeg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_By57N4.jpeg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_EJWqGf.jpeg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_EUJ6CX.jpeg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_FtYRqz.jpeg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_HuMwmi.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_J7hL2Y.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_NGiOVN.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_SuY5t1.jpeg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_YevJzo.jpeg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_Zwe8i7.jpeg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_bCoAZg.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_bouUUL.jpeg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_f22xL0.jpeg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_jbDUGG.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_jkpHUf.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_k7E9yz.jpeg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_krz5Kc.jpeg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_mhIu8M.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_oOXqz9.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_q0MS9f.jpeg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_sZNOcT.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_soB3nB.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_teXsR2.jpeg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_uKHtOl.jpeg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_uiDeaI.jpg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_unaCV7.jpeg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_y31JbP.jpeg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_yol9H8.jpeg: corrupt JPEG restored and saved\n\u001b[34m\u001b[1mval: \u001b[0m/kaggle/working/datasets/val/images/fold_2/ID_z6gC1L.jpg: corrupt JPEG restored and saved\n","output_type":"stream"},{"name":"stderr","text":"\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 35/35 [00:17<00:00,  1.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        553        943      0.791      0.736      0.818      0.571\n               healthy        172        386      0.768      0.725      0.808      0.551\n           anthracnose        136        194      0.785      0.784      0.852      0.613\n                 cssvd        246        363      0.819        0.7      0.793      0.548\nSpeed: 0.2ms preprocess, 23.7ms inference, 0.0ms loss, 1.4ms postprocess per image\nResults saved to \u001b[1mruns/detect/val2\u001b[0m\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"Our Cross Validation is at 0.818 which is slightly lower than the first model but is great. Let's now infer the models","metadata":{}},{"cell_type":"markdown","source":"Make sure to save both models for the inference in the next notebook","metadata":{}},{"cell_type":"markdown","source":"We are so sorry we could not show the train logs for the code review. We were short on time.\nThis training time is slightly below the 9 hour training time.","metadata":{}},{"cell_type":"markdown","source":"### Complete this notebook with the inference and explainability notebook","metadata":{}}]}